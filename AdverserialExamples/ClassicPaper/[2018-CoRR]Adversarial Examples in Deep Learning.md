# Adversarial Examples in Deep Learning: Characterization and Divergence
## 摘要
- 深度学习任务 <-> 敏感数据  <-> 安全隐私
- 深度学习中的对抗攻击 <-> 最重要的安全威胁
- 文章工作：深度学习中的对抗性例子进行统计分析
  - 对抗攻击形式化描述，原理，分类，效果评估
  - 在不同模型结构、参数的情况下，模拟对抗攻击
  - 模拟结果，攻击效果受模型结构、参数影响

## 1. 介绍
- 第一段
  - 深度学习很成功
- 第二段
  - 深度学习模型也很脆弱，预测阶段受到对抗输入的影响
    - 对抗样本是什么
    - 对抗样本的目的
- 第三段
  - 对抗样本催生了大量相关研究
    - 黑盒攻击，可迁移特性
    - 训练、预测阶段，均有可能发生攻击
- 第四段
  - 深入了解分析对抗样本攻击，是有效环节、防御对抗攻击的第一步
## 2. 对抗样本与攻击
### 2.1 深度神经网络模型
- 第一段
  - 输入 -> DNN -> 输出
- 第二段
  - 模型部署两阶段：训练、预测
- 第三段
  - 具体训练、预测步骤
### 2.2 威胁模型
- 内部威胁：白盒攻击，向训练集中注入对抗样本
- 外部威胁：黑盒攻击
  - 非定向攻击：只要分类错误即可
  - 定向攻击：分类到指定的错误类别
- 对抗样本攻击流程：7步骤
  - 将正常输入x发送至预测API
  - 训练好的模型计算出预测结果
  - API返回预测类别及类别概率
  - 攻击者拦截预测结果
  - 生成对抗样本$x_{adv}$
  - 攻击者收集对抗样本对应的预测类别及类别概率，直到攻击成功
  - 用户收到错误结果
### 2.3 对抗样本形式化描述
- 理想的对抗样本
  - 既能干扰识别结果，又能与正常输入相比波动最小
  - 样本距离：L0（更改像素数），L2（欧氏距离）
### 2.4 对抗样本的影响
- 评价指标：成功率（SR），改变度（DoC），信息熵（IE）
  - SR，反应了将输入从原类别改变到错误类别的难度
  - DoC，反应了对抗样本与正常样本之间的距离，可以理解为改变的像素数
  - IE，反应了随机数据源产生的信息量，可以理解为不确定性
### 2.5 干扰注入的原则
- 干扰的目的：通过改变目标在对抗样本方向上的梯度值进而改变目标函数值，使DNN模型对输入进行误分类
- 干扰的方式：一步或多步实现
